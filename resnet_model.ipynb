{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729b3d8e",
   "metadata": {},
   "source": [
    "# Signature Verification Model\n",
    "\n",
    "Objective: Differentiates forged offline signatures from their original counterparts.\n",
    "\n",
    "`ResNet` pre-trained model to choose from:\n",
    "\n",
    "- `ResNet18`\n",
    "- `ResNet34`\n",
    "- `ResNet50`\n",
    "- `ResNet101`\n",
    "- `ResNet152`\n",
    "\n",
    "This model uses a Triplet Loss Function to aid in differentiating forged signatures from the originals.\n",
    "\n",
    "Dataset: [CEDAR](https://www.kaggle.com/datasets/shreelakshmigp/cedardataset)\n",
    "\n",
    "PyTorch documentation: [PyTorch Documentation](https://docs.pytorch.org/docs/stable/index.html)\n",
    "\n",
    "PyTorch installed: \n",
    "```bash\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad577ad1",
   "metadata": {},
   "source": [
    "# Notebook Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e42b2f",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, Any, List, DefaultDict, Callable\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models # type: ignore\n",
    "import torchvision.transforms as transforms # type: ignore\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.models import get_model_weights # type: ignore\n",
    "from torch import autocast, GradScaler\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc, confusion_matrix # type: ignore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold # type: ignore\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go # type: ignore\n",
    "\n",
    "# Visualisation\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d36202",
   "metadata": {},
   "source": [
    "## Seeding\n",
    "\n",
    "> Did you know? 42 is a reference to Douglas Adams's The Hitchhiker Guide to the Galaxy!\n",
    "> In the book, the supercomputer Deep Thought reveals that 42 is the answer to the great question of “life, the universe and everything”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Ensure reproducibility\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  # type:ignore\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_SEED = 42\n",
    "set_seed(FIXED_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1472e14e",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "Modify these to control how the fine-tuning of the model goes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CONFIG: Dict[str, str] = {\"DATASET_PATH\": \"processed_signature_images\"}\n",
    "\n",
    "LEARNING_CONFIG: Dict[str, str | int | float] = {\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"EPOCH\": 150,\n",
    "    \"LEARNING_RATE\": 1e-3,\n",
    "    \"EMBEDDING_DIM\": 256,\n",
    "    # \"NUM_CLASSES\": 2 ,\n",
    "    \"EARLY_STOPPING_PATIENT\": 10,\n",
    "    \"CHECKPOINT_DIR\": \"checkpoint/\",\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"GRAD_CLIP\": 1.0,\n",
    "    \"K_FOLDS\": 5,\n",
    "}\n",
    "\n",
    "OPTIMISER_PARAMS: Dict[str, str | Tuple[float, float] | float] = {\n",
    "    \"optimiser\": \"Adam\",\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"weight_decay\": 1e-4,\n",
    "}\n",
    "\n",
    "SCHEDULER_PARAMS: Dict[str, str | float | int] = {\n",
    "    \"scheduler\": \"OneCycleLR\",\n",
    "    \"max_lr\": 1e-3,\n",
    "    \"epochs\": LEARNING_CONFIG[\"EPOCH\"],\n",
    "    \"pct_start\": 0.2,\n",
    "    \"div_factor\": 10.0,\n",
    "    \"final_div_factor\": 10000.0,\n",
    "}\n",
    "\n",
    "BACKBONE_CONFIG: Dict[str, Dict[str, Any]] = {\n",
    "    \"resnet18\": {\"builder\": models.resnet18, \"out_channels\": 512},\n",
    "    \"resnet34\": {\"builder\": models.resnet34, \"out_channels\": 512},\n",
    "    \"resnet50\": {\"builder\": models.resnet50, \"out_channels\": 2048},\n",
    "    \"resnet101\": {\"builder\": models.resnet101, \"out_channels\": 2048},\n",
    "    \"resnet152\": {\"builder\": models.resnet152, \"out_channels\": 2048},\n",
    "    # Add other models if needed, and determine their output channels before pooling\n",
    "    # Example for a different model:\n",
    "    # 'vgg16': {'builder': models.vgg16, 'out_channels': 512},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e41710",
   "metadata": {},
   "source": [
    "# Functions and Classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2cf918",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Definition of neural network model. \n",
    "\n",
    "The model's first layer has been modified to accept grayscale images instead of RGB images and the classification layer is removed. \n",
    "\n",
    "This version uses `ResNet` pre-trained model from PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction(nn.Module):\n",
    "    \"\"\"\n",
    "    FeatureExtration is a module that uses pre-trained ResNet models from PyTorch to extract feature embeddings from signature images.\n",
    "\n",
    "    The pre-trained model has its classification layer removed, as it is not useful here.\n",
    "\n",
    "    The first layer has been modified to accept grayscale images instead of RGB images\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "\n",
    "    embedding_dim: int\n",
    "        Dimension of the output embedding.\n",
    "    weights: str\n",
    "        String value for the weight (e.g. : IMAGENET1K_V1)\n",
    "    backbone_type: str\n",
    "        Indicate the type of pre-trained model to use\n",
    "    extra_channels: Optional[List[int]]\n",
    "        Control the number of extra channels based on the size of the embeddings\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "    _initialise_custom_weights(self):\n",
    "        Initializes weights for custom layers in the model to ensure proper weight distribution for improved training performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 256,\n",
    "        weights: Optional[str] = None,\n",
    "        use_extra_layers: bool = True,\n",
    "        backbone_type: str = \"resnet50\",\n",
    "        extra_channels: Optional[List[int]] = None,\n",
    "        dropout_rate: float = 0.3,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__() # type: ignore\n",
    "\n",
    "        if embedding_dim <= 0:\n",
    "            raise ValueError(\n",
    "                f\"Embedding dimension must be positive, got {embedding_dim}\"\n",
    "            )\n",
    "        if dropout_rate < 0 or dropout_rate > 1:\n",
    "            raise ValueError(\n",
    "                f\"Dropout rate must be between 0 and 1, got {dropout_rate}\"\n",
    "            )\n",
    "        if backbone_type not in BACKBONE_CONFIG:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported backbone type: {backbone_type}. Choose from {list(BACKBONE_CONFIG.keys())}\"\n",
    "            )\n",
    "        if extra_channels is not None and not all(c > 0 for c in extra_channels):\n",
    "            raise ValueError(\n",
    "                f\"extra_channels must be a list of positive integers or None, got {extra_channels}\"\n",
    "            )\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_extra_layers = use_extra_layers\n",
    "        self.backbone_type = backbone_type\n",
    "        self.extra_channels = extra_channels if extra_channels is not None else []\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        backbone_builder = BACKBONE_CONFIG[self.backbone_type][\"builder\"]\n",
    "        backbone_out_channels = BACKBONE_CONFIG[self.backbone_type][\"out_channels\"]\n",
    "\n",
    "        weights_enum = None\n",
    "        if weights is not None:\n",
    "            try:\n",
    "                weights_enum_type = get_model_weights(backbone_builder)\n",
    "\n",
    "                if weights_enum_type is None: # type: ignore\n",
    "                    print(\n",
    "                        f\"Warning: Could not get weights type for backbone '{self.backbone_type}'. Using default random initialisation.\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "                    weights = None\n",
    "                else:\n",
    "                    weights_enum = getattr(weights_enum_type, weights)\n",
    "\n",
    "            except AttributeError:\n",
    "                print(\n",
    "                    f\"Warning: Specified weights alias '{weights}' not found for {self.backbone_type}. Check available weights in torchvision documentation. Using default random initialisation.\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "                weights = None\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: An unexpected error occurred looking up weights '{weights}' for {self.backbone_type}: {e}. Using default random initialisation.\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "                weights = None\n",
    "\n",
    "        self.model = backbone_builder(weights=weights_enum)\n",
    "\n",
    "        original_conv1 = self.model.conv1\n",
    "\n",
    "        if original_conv1.in_channels == 3:\n",
    "            self.model.conv1 = nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=original_conv1.out_channels,\n",
    "                kernel_size=original_conv1.kernel_size,\n",
    "                stride=original_conv1.stride,\n",
    "                padding=original_conv1.padding,\n",
    "                bias=original_conv1.bias is not None,\n",
    "            )\n",
    "            if weights is not None:\n",
    "                with torch.no_grad():\n",
    "                    self.model.conv1.weight.data = original_conv1.weight.data.mean(\n",
    "                        dim=1, keepdim=True\n",
    "                    )\n",
    "            else:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    self.model.conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\"\n",
    "                )\n",
    "                if self.model.conv1.bias is not None:\n",
    "                    nn.init.constant_(self.model.conv1.bias, 0)\n",
    "\n",
    "        self.backbone = nn.Sequential(*list(self.model.children())[:-2])\n",
    "\n",
    "        self.extra_layers = None\n",
    "        if self.use_extra_layers and len(self.extra_channels) > 0:\n",
    "            extra_layer_list: List[nn.Module] = []\n",
    "            current_in_channels = backbone_out_channels\n",
    "\n",
    "            for _, output_c in enumerate(self.extra_channels):\n",
    "                extra_layer_list.append(\n",
    "                    nn.Conv2d(\n",
    "                        current_in_channels,\n",
    "                        output_c,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                        bias=False,\n",
    "                    )\n",
    "                )\n",
    "                extra_layer_list.append(nn.BatchNorm2d(output_c))\n",
    "                extra_layer_list.append(nn.ReLU(inplace=True))\n",
    "                current_in_channels = output_c\n",
    "\n",
    "            extra_layer_list.append(\n",
    "                nn.Conv2d(\n",
    "                    current_in_channels,\n",
    "                    self.embedding_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "            extra_layer_list.append(nn.BatchNorm2d(self.embedding_dim))\n",
    "            extra_layer_list.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            self.extra_layers = nn.Sequential(*extra_layer_list)\n",
    "\n",
    "        final_fc_in_features = (\n",
    "            self.embedding_dim\n",
    "            if (self.use_extra_layers and len(self.extra_channels) > 0)\n",
    "            else backbone_out_channels\n",
    "        )\n",
    "\n",
    "        self.final_processing = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(final_fc_in_features),\n",
    "            nn.Linear(final_fc_in_features, self.embedding_dim),\n",
    "            nn.BatchNorm1d(self.embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "        )\n",
    "\n",
    "        self._initialise_custom_weights()\n",
    "\n",
    "    def _initialise_custom_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes weights for custom layers in the model.\n",
    "\n",
    "        - Uses Kaiming Normal initialization for Conv2D and Linear layers.\n",
    "        - Sets biases to zero for stability.\n",
    "        - Initializes BatchNorm layers with weights of 1 and biases of 0.\n",
    "\n",
    "        This ensures proper weight distribution for improved training performance.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.extra_layers is not None:\n",
    "            for m in self.extra_layers.modules():\n",
    "                if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                    nn.init.kaiming_normal_(\n",
    "                        m.weight, mode=\"fan_out\", nonlinearity=\"relu\"\n",
    "                    )\n",
    "                    if hasattr(m, \"bias\") and m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        for m in self.final_processing.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if hasattr(m, \"bias\") and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim != 4:\n",
    "            raise ValueError(\n",
    "                f\"Expected input tensor to be 4D (batch, channels, H, W), but got {x.ndim}D\"\n",
    "            )\n",
    "\n",
    "        if x.size(1) != self.model.conv1.in_channels:\n",
    "            raise ValueError(\n",
    "                f\"Input tensor has {x.size(1)} channels, but the model expects {self.model.conv1.in_channels} channels.\"\n",
    "            )\n",
    "\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        if self.use_extra_layers and self.extra_layers is not None:\n",
    "            features = self.extra_layers(features)\n",
    "\n",
    "        embeddings = self.final_processing(features)\n",
    "\n",
    "        return F.normalize(embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1374784",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The chosen loss function is the **Triplet Loss Function**, which works by measuring distances between anchor nodes and positive nodes, as well as between anchor nodes and negative nodes. By applying this loss function, the model learns to widen the distances between negative nodes and  anchor nodes, while minimising distances between anchor nodes and positive nodes. \n",
    "\n",
    "This loss function aligns perfectly with verification tasks, such as signature verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Implements triplet loss function with batch-wise minig strategies.\n",
    "\n",
    "    Supports batch hard and batch semi-hard mining strategies.\n",
    "\n",
    "    Supports Euclidean and cosine distance metrics.\n",
    "\n",
    "    [Optional] Diversity regularisation\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    margin: float\n",
    "        Non-negative margin value for the triplet loss function.\n",
    "    mining_strategy: str\n",
    "        'batch_semi_hard' or 'batch_hard'. Default is 'batch_semi_hard'.\n",
    "    distance_metric: str\n",
    "        'euclidean' or 'cosine'. Default is 'euclidean'.\n",
    "    soft_margin: bool\n",
    "    lambda_diversity: float\n",
    "        Diversity regularisation weight\n",
    "    use_diversity: bool\n",
    "    p: int\n",
    "        p-norm in Euclidean distance computation\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "    _compute_euclidean_distance(self, embedding_one: torch.Tensor, embedding_two: torch.Tensor) -> torch.Tensor:\n",
    "        Computes the Euclidean distance between two tensors\n",
    "    _compute_cosine_distance(self, embedding_one: torch.Tensor, embedding_two: torch.Tensor) -> torch.Tensor:\n",
    "        Computes the Cosine distance between two tensors\n",
    "    _get_triplet_mask(self, labels: torch.Tensor) -> Tuple:\n",
    "        Generate boolean masks for positive and negative pairs based on their labels\n",
    "    _batch_hard_mining(self, distances: torch.Tensor, mask_anchor_positive: torch.Tensor, mask_anchor_negative: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        Performs hard mining; selects hardest positive and negative pairs\n",
    "    _batch_semi_hard_mining(self, distance: torch.Tensor, mask_anchor_positive: torch.Tensor, mask_anchor_negative: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        Performs semi-hard mining; selects moderately difficult positive and negative pairs\n",
    "    _compute_diversity_regularisation(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        Computes diversity regularisation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        margin: float = 1.0,\n",
    "        mining_strategy: str = \"batch_semi_hard\",\n",
    "        distance_metric: str = \"euclidean\",\n",
    "        soft_margin: bool = False,\n",
    "        lambda_diversity: float = 0.1,\n",
    "        use_diversity: bool = True,\n",
    "        p: int = 2,\n",
    "        normalise_embeddings: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__() # type: ignore\n",
    "\n",
    "        if margin < 0:\n",
    "            raise ValueError(f\"Margin must be non-negative, got {margin}\")\n",
    "        if mining_strategy not in [\"batch_hard\", \"batch_semi_hard\"]:\n",
    "            raise ValueError(f\"Invalid mining strategy, got {mining_strategy}\")\n",
    "        if distance_metric not in [\"euclidean\", \"cosine\"]:\n",
    "            raise ValueError(f\"Invalid distance metric, got {distance_metric}\")\n",
    "\n",
    "        self.margin = margin\n",
    "        self.mining_strategy = mining_strategy\n",
    "        self.distance_metric = distance_metric\n",
    "        self.soft_margin = soft_margin\n",
    "        self.lambda_diversity = lambda_diversity\n",
    "        self.use_diversity = use_diversity\n",
    "        self.p = p\n",
    "        self.normalise_embeddings = normalise_embeddings\n",
    "\n",
    "    def _compute_euclidean_distance(\n",
    "        self, embedding_one: torch.Tensor, embedding_two: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Computes the Euclidean distance between two tensors.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            embedding_one: torch.Tensor\n",
    "                The embedding of the original or forged signature.\n",
    "            embedding_two: torch.Tensor\n",
    "                The embedding of the anchor signature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            torch.Tensor\n",
    "                The Euclidean distance value\n",
    "\n",
    "        The function accepts the embeddings of both original or forged signature image and an anchor image,\n",
    "        calculates the Euclidean distance, and returns it as a tensor.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return F.pairwise_distance(embedding_one, embedding_two, p=self.p)\n",
    "\n",
    "    def _compute_cosine_distance(\n",
    "        self, embedding_one: torch.Tensor, embedding_two: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Computes the Cosine distance between two tensors.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            embedding_one: torch.Tensor\n",
    "                The embedding of the original or forged signature.\n",
    "            embedding_two: torch.Tensor\n",
    "                The embedding of the anchor signature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            torch.Tensor\n",
    "                The Cosine distance value\n",
    "\n",
    "        The function accepts the embeddings of both original or forged signature image and an anchor image,\n",
    "        calculates the Cosine distance, and returns it as a tensor.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return 1 - F.cosine_similarity(embedding_one, embedding_two, dim=-1)\n",
    "\n",
    "    def _get_triplet_mask(\n",
    "        self, labels: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "\n",
    "        Generate boolean masks for positive and negative pairs based on their labels\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "\n",
    "        labels: torch.Tensor\n",
    "            The signers' ids\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            The masks for the pairs\n",
    "\n",
    "        This function accepts the labels (signers' ids), marks the pairs as positive or negative\n",
    "        to enforce similarity and dissimilarity, and returns the masks in a tuple.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        labels = labels.unsqueeze(1)\n",
    "        mask_anchor_positive = (labels == labels.T) & ~torch.eye(\n",
    "            labels.size(0), dtype=bool, device=labels.device # type: ignore\n",
    "        )\n",
    "        mask_anchor_negative = labels != labels.T\n",
    "\n",
    "        return (mask_anchor_positive, mask_anchor_negative)\n",
    "\n",
    "    def _batch_hard_mining(\n",
    "        self,\n",
    "        distances: torch.Tensor,\n",
    "        mask_anchor_positive: torch.Tensor,\n",
    "        mask_anchor_negative: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "\n",
    "        Performs hard mining by selecting hardest positive and negative pairs\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "\n",
    "        distances: torch.Tensor\n",
    "            Distance calculated with Euclidean or Cosine distance metric.\n",
    "        mask_anchor_positive: torch.Tensor\n",
    "            Mask indicating pairs with the same signer.\n",
    "        mask_anchor_negative: torch.Tensor\n",
    "            Mask indicating pairs with different signers.\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            Largest positive distance and smallest negative distance\n",
    "\n",
    "        The function accepts distances between embeddings and their masks to compute\n",
    "        the largest positive distance and the smallest negative distance.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        positive_distance = torch.where(\n",
    "            mask_anchor_positive, distances, torch.full_like(distances, -float(\"inf\"))\n",
    "        )\n",
    "        max_positive_distance, _ = torch.max(positive_distance, dim=1)\n",
    "\n",
    "        negative_distance = torch.where(\n",
    "            mask_anchor_negative, distances, torch.full_like(distances, float(\"inf\"))\n",
    "        )\n",
    "\n",
    "        min_negative_distance, _ = torch.min(negative_distance, dim=1)\n",
    "\n",
    "        return max_positive_distance, min_negative_distance\n",
    "\n",
    "    def _batch_semi_hard_mining(\n",
    "        self,\n",
    "        distances: torch.Tensor,\n",
    "        mask_anchor_positive: torch.Tensor,\n",
    "        mask_anchor_negative: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "\n",
    "        Performs semi-hard mining by selecting hardest positive and semi-hard pairs\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "\n",
    "        distances: torch.Tensor\n",
    "            Distance calculated with Euclidean or Cosine distance metric.\n",
    "        mask_anchor_positive: torch.Tensor\n",
    "            Mask indicating pairs with the same signer.\n",
    "        mask_anchor_negative: torch.Tensor\n",
    "            Mask indicating pairs with different signers.\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            Largest positive distance and semi-hard negative distance\n",
    "\n",
    "        This function filters positive and negative distances while enforcing:\n",
    "        - The hardest positive sample as the one with the largest distance.\n",
    "        - The semi-hard negative sample as the hardest negative that is:\n",
    "            1. Harder than the positive distance (`d(a,n) > d(a,p)`)\n",
    "            2. Easier than the positive distance plus a margin (`d(a,n) < d(a,p) + margin`)\n",
    "        - If no valid semi-hard negatives exist, it falls back to selecting the hardest overall negative.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        positive_distance_filtered = torch.where(\n",
    "            mask_anchor_positive, distances, torch.full_like(distances, -float(\"inf\"))\n",
    "        )\n",
    "        hardest_positive_dist, _ = torch.max(positive_distance_filtered, dim=1)\n",
    "\n",
    "        valid_negative_distances = torch.where(\n",
    "            mask_anchor_negative, distances, torch.full_like(distances, float(\"inf\"))\n",
    "        )\n",
    "\n",
    "        # Condition 1: d(a,n) > d(a,p)  (Negative is harder than positive)\n",
    "        is_harder_than_positive = (\n",
    "            valid_negative_distances > hardest_positive_dist.unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        # Condition 2: d(a,n) < d(a,p) + margin (Negative is easier than (positive + margin))\n",
    "        is_easier_than_margin = valid_negative_distances < (\n",
    "            hardest_positive_dist.unsqueeze(1) + self.margin\n",
    "        )\n",
    "\n",
    "        semi_hard_mask = (\n",
    "            mask_anchor_negative & is_harder_than_positive & is_easier_than_margin\n",
    "        )\n",
    "\n",
    "        semi_hard_negative_distances_filtered = torch.where(\n",
    "            semi_hard_mask, distances, torch.full_like(distances, float(\"inf\"))\n",
    "        )\n",
    "\n",
    "        easiest_semi_hard_negative_dist, _ = torch.min(\n",
    "            semi_hard_negative_distances_filtered, dim=1\n",
    "        )\n",
    "\n",
    "        hardest_overall_negative_dist, _ = torch.min(valid_negative_distances, dim=1)\n",
    "\n",
    "        selected_negative_dist = torch.where(\n",
    "            semi_hard_mask.sum(dim=1) > 0,\n",
    "            easiest_semi_hard_negative_dist,\n",
    "            hardest_overall_negative_dist,\n",
    "        )\n",
    "\n",
    "        return hardest_positive_dist, selected_negative_dist\n",
    "\n",
    "    def _compute_diversity_regularisation(\n",
    "        self, embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Computes diveristy regularisation for embeddings.\n",
    "\n",
    "        This function works by penalising excessive similarity betwen embeddings\n",
    "        by calculating the squared Frobenius norm of the difference between\n",
    "        the similarity matrix and the identity matrix. The goal is to encourage\n",
    "        representations that are spread out in the embedding space.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "\n",
    "        embeddings: torch.Tensor\n",
    "            A tensor containing the embeddings to be regularised.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The diversity regularisation loss scaled by `self.lambda_diveristy`.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        normalised_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        similarity_matrix = normalised_embeddings @ normalised_embeddings.T\n",
    "\n",
    "        identity_matrix = torch.eye(\n",
    "            embeddings.size(0), device=embeddings.device, dtype=embeddings.dtype\n",
    "        )\n",
    "\n",
    "        diversity_loss = (similarity_matrix - identity_matrix).pow(2).sum() / (\n",
    "            embeddings.size(0) * (embeddings.size(0) - 1)\n",
    "        )\n",
    "\n",
    "        return self.lambda_diversity * diversity_loss\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = embeddings.size(0) # type: ignore\n",
    "\n",
    "        if self.normalise_embeddings:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if self.distance_metric == \"euclidean\":\n",
    "            pairwise_distance: torch.Tensor = self._compute_euclidean_distance(\n",
    "                embeddings.unsqueeze(1), embeddings.unsqueeze(0)\n",
    "            )\n",
    "        elif self.distance_metric == \"cosine\":\n",
    "            pairwise_distance: torch.Tensor = self._compute_cosine_distance(\n",
    "                embeddings.unsqueeze(1), embeddings.unsqueeze(0)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid distance metrics: {self.distance_metric}\")\n",
    "\n",
    "        mask_anchor_positive, mask_anchor_negative = self._get_triplet_mask(labels)\n",
    "\n",
    "        if self.mining_strategy == \"batch_hard\":\n",
    "            positive_distance, negative_distance = self._batch_hard_mining(\n",
    "                pairwise_distance, mask_anchor_positive, mask_anchor_negative\n",
    "            )\n",
    "        elif self.mining_strategy == \"batch_semi_hard\":\n",
    "            positive_distance, negative_distance = self._batch_semi_hard_mining(\n",
    "                pairwise_distance, mask_anchor_positive, mask_anchor_negative\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mining strategy: {self.mining_strategy}\")\n",
    "\n",
    "        valid_triplets_mask = ~(\n",
    "            torch.isinf(positive_distance) | torch.isinf(negative_distance)\n",
    "        )\n",
    "        positive_distance = positive_distance[valid_triplets_mask]\n",
    "        negative_distance = negative_distance[valid_triplets_mask]\n",
    "\n",
    "        if positive_distance.numel() == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)\n",
    "\n",
    "        if self.soft_margin:\n",
    "            triplet_loss = F.softplus(positive_distance - negative_distance)\n",
    "        else:\n",
    "            triplet_loss = F.relu(positive_distance - negative_distance + self.margin)\n",
    "\n",
    "        if self.use_diversity and self.lambda_diversity > 0:\n",
    "            diversity_loss = self._compute_diversity_regularisation(embeddings)\n",
    "        else:\n",
    "            diversity_loss = 0\n",
    "\n",
    "        total_loss = triplet_loss.mean() + diversity_loss\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4a650",
   "metadata": {},
   "source": [
    "## Dataset Classes\n",
    "\n",
    "These classes are responsible for loading and preprocessing signature image data along with its metadata (signer IDs, image types) for the subsequent fine-tuning. \n",
    "\n",
    "They provide a structured output of a `torch.Tensor` for the image, a string for the signer's identity, and another string to indicate the image type ('original' or 'forged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0aa261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSignatureDataset(Dataset[Tuple[torch.Tensor, str, str]]):\n",
    "    \"\"\"\n",
    "\n",
    "    Loads and handles signature images\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    data_map: Dict[str, Dict[str, List[str]]]\n",
    "        A dictionary that maps the signature images to the signer for both original images and forgeries\n",
    "    transform: Optional[transforms.Compose]\n",
    "        Transformation to apply to the images\n",
    "\n",
    "    Mehods\n",
    "    ------\n",
    "\n",
    "    ___len__(self) -> int:\n",
    "        Returns the total number of signature images\n",
    "    __getitem__(self, index: int) -> Any:\n",
    "        Retrieves an image tensor and its associated signer id.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_map: Dict[str, Dict[str, List[str]]],\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "    ) -> None:\n",
    "        self.data_map = data_map\n",
    "        self.transform = transform\n",
    "        self.signer_ids = sorted(list(data_map.keys()), key=int)\n",
    "\n",
    "        self.all_image_references: List[Tuple[str, str, int]] = []\n",
    "        for signer_id in self.signer_ids:\n",
    "            for index, _ in enumerate(data_map[signer_id].get(\"original\", [])):\n",
    "                self.all_image_references.append((signer_id, \"original\", index))\n",
    "\n",
    "            for index, _ in enumerate(data_map[signer_id].get(\"forged\", [])):\n",
    "                self.all_image_references.append((signer_id, \"forged\", index))\n",
    "\n",
    "        print(f\"--- Inside TrainingSignatureDataset.__init__ ---\")\n",
    "        print(f\"  Length of self.data_map (signer_ids): {len(self.data_map)}\")\n",
    "        print(\n",
    "            f\"  Length of self.all_image_references after population: {len(self.all_image_references)}\"\n",
    "        )\n",
    "        print(f\"----------------------------------------\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.all_image_references)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        \"\"\"\n",
    "        \n",
    "        Retrieve an image tensor and its associated signer ID.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            index: int\n",
    "                Index of the image reference in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - image_tensor: The grayscale image converted to a PyTorch tensor.\n",
    "                - signer_label_tensor: A tensor representing the signer ID.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "            ValueError: If the image type is unknown.\n",
    "\n",
    "        The function retrieves an image path based on the signer ID and image type ('original' or 'forged'),\n",
    "        loads it as a grayscale image, applies transformations if available, and returns it as a tensor\n",
    "        along with the corresponding signer label.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        signer_id, image_type, image_index = self.all_image_references[index]\n",
    "        if image_type == \"original\":\n",
    "            path = self.data_map[signer_id][\"original\"][image_index]\n",
    "        elif image_type == \"forged\":\n",
    "            path = self.data_map[signer_id][\"forged\"][image_index]\n",
    "        else:\n",
    "            raise ValueError(f\"Uknown image type: {image_type}\")\n",
    "\n",
    "        image_pil = Image.open(path)\n",
    "        # 'L' is for grayscale\n",
    "        if image_pil.mode != \"L\":\n",
    "            image_pil = image_pil.convert(\"L\")\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_pil)  # type: ignore\n",
    "        else:\n",
    "            image_tensor = torch.from_numpy(image_pil).unsqueeze(0).float() / 255.0  # type: ignore\n",
    "\n",
    "        signer_label_tensor = torch.tensor(int(signer_id), dtype=torch.long)\n",
    "\n",
    "        return image_tensor, signer_label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingSignatureDataset(Dataset[Tuple[torch.Tensor, str, str]]):\n",
    "    \"\"\"\n",
    "\n",
    "    Prepare offline triplets for testing\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    data_map: Dict[str, Dict[str, List[str]]]\n",
    "        A dictionary that maps the signature images to the signer for both original images and forgeries\n",
    "    transform: Optional[transforms.Compose]\n",
    "        Transformation to apply to the images\n",
    "\n",
    "    Mehods\n",
    "    ------\n",
    "\n",
    "    ___len__(self) -> int:\n",
    "        Returns the total number of signature images\n",
    "    __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n",
    "        Returns anchor, positive, negative, and signer id\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_map: Dict[str, Dict[str, List[str]]],\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "    ):\n",
    "        self.data_map = data_map\n",
    "        self.transform = transform\n",
    "\n",
    "        self.all_signers_ids: List[str] = sorted(list(data_map.keys()), key=int)\n",
    "\n",
    "        self.anchor_candidates: List[Tuple[str, str]] = []\n",
    "\n",
    "        for signer_id, signature_type in data_map.items():\n",
    "            paths_for_signer = signature_type.get(\"original\", []) + signature_type.get(\n",
    "                \"forged\", []\n",
    "            )\n",
    "\n",
    "            for path in paths_for_signer:\n",
    "                self.anchor_candidates.append((path, signer_id))\n",
    "\n",
    "        self.all_signers_ids_filtered: List[str] = []\n",
    "\n",
    "        for signer_id in self.all_signers_ids:\n",
    "            total_images_for_signer = len(\n",
    "                data_map[signer_id].get(\"original\", [])\n",
    "            ) + len(data_map[signer_id].get(\"forged\", []))\n",
    "\n",
    "            if total_images_for_signer >= 2:\n",
    "                self.all_signers_ids_filtered.append(signer_id)\n",
    "\n",
    "        if len(self.all_signers_ids_filtered) < 2:\n",
    "            raise ValueError(\n",
    "                \"Not enough distinct signers with at least two images each to form valid triplets. \"\n",
    "                \"Ensure your dataset has at least two signers, each with >= 2 images for testing.\"\n",
    "            )\n",
    "\n",
    "        print(f\"--- Inside TrainingSignatureDataset.__init__ ---\")\n",
    "        print(f\"  Length of self.data_map (signer_ids): {len(self.data_map)}\")\n",
    "        print(f\"----------------------------------------\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor_candidates)\n",
    "\n",
    "    def __getitem__(  # type: ignore\n",
    "        self, index: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Forms triplets\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            index (int): Index of the image reference in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - image_tensor: The grayscale image converted to a PyTorch tensor.\n",
    "                - signer_label_tensor: A tensor representing the signer ID.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "            ValueError: If the image type is unknown.\n",
    "\n",
    "        The function retrieves an image path based on the signer ID and image type ('original' or 'forged'),\n",
    "        loads it as a grayscale image, applies transformations if available, and returns the triplets as tensors\n",
    "        along with the corresponding signer ID.\n",
    "        \"\"\"\n",
    "\n",
    "        anchor_path: str\n",
    "        anchor_signer_id: str\n",
    "\n",
    "        anchor_path, anchor_signer_id = self.anchor_candidates[index]\n",
    "        anchor_label_int: int = int(anchor_signer_id)\n",
    "\n",
    "        all_paths_for_anchor = self.data_map[anchor_signer_id].get(\n",
    "            \"original\", []\n",
    "        ) + self.data_map[anchor_signer_id].get(\"forged\", [])\n",
    "\n",
    "        positive_candidates = [p for p in all_paths_for_anchor if p != anchor_path]\n",
    "\n",
    "        if not positive_candidates:\n",
    "            print(\n",
    "                f\"Warning: Signer {anchor_signer_id} has insufficient images to select a distinct positive. Resampling index...\"\n",
    "            )\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        positive_path = random.choice(positive_candidates)\n",
    "\n",
    "        negative_paths_candidates: List[str] = []\n",
    "\n",
    "        forgeries_of_anchor = self.data_map[anchor_signer_id].get(\"forged\", [])\n",
    "        negative_paths_candidates.extend(forgeries_of_anchor)\n",
    "\n",
    "        other_signer_ids = [\n",
    "            aid for aid in self.all_signers_ids_filtered if aid != anchor_signer_id\n",
    "        ]\n",
    "\n",
    "        if not other_signer_ids and not forgeries_of_anchor:\n",
    "            raise ValueError(\n",
    "                f\"No eligible negative signers found for anchor_signer_id {anchor_signer_id}. \"\n",
    "                \"The dataset might contain too few unique signers.\"\n",
    "            )\n",
    "\n",
    "        if other_signer_ids:\n",
    "            for other_aid in other_signer_ids:\n",
    "                negative_paths_candidates.extend(\n",
    "                    self.data_map[other_aid].get(\"original\", [])\n",
    "                    + self.data_map[other_aid].get(\"forged\", [])\n",
    "                )\n",
    "\n",
    "        if not negative_paths_candidates:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        negative_path = random.choice(negative_paths_candidates)\n",
    "\n",
    "        # Assuming images are grayscale\n",
    "        anchor_img = Image.open(anchor_path).convert(\"L\")\n",
    "        positive_img = Image.open(positive_path).convert(\"L\")\n",
    "        negative_img = Image.open(negative_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_tensor = self.transform(anchor_img)  # type: ignore\n",
    "            positive_tensor = self.transform(positive_img)  # type: ignore\n",
    "            negative_tensor = self.transform(negative_img)  # type: ignore\n",
    "        else:\n",
    "            anchor_tensor = transforms.ToTensor()(anchor_img)\n",
    "            positive_tensor = transforms.ToTensor()(positive_img)\n",
    "            negative_tensor = transforms.ToTensor()(negative_img)\n",
    "\n",
    "        return anchor_tensor, positive_tensor, negative_tensor, anchor_label_int  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515cf24",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "These functions are made to assist the process of loading signature images, creating datasets and dataloaders, and the evaluation of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a67b2c",
   "metadata": {},
   "source": [
    "### Preparing Signature Images\n",
    "\n",
    "For these functions to work, the name images' names should be as follows:\n",
    "\n",
    "\\<original/forgeries>\\_\\<signer's id>\\_\\<image's index>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signer_id(file_name: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "    Extracts the first sequence of digits from a given file name and returns it as an integer.\n",
    "\n",
    "    If no number is found, it defaults to 'UNKNOWN_SIGNER'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name: str\n",
    "        The name of the file in string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The extracted number as a string. If no number is found, return 'UNKOWN_SIGNER'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r\"(?:original|forgeries)_(\\d+)_\", file_name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"UNKNOWN_SIGNER\"\n",
    "\n",
    "\n",
    "# One liner because why not\n",
    "\n",
    "# extract_author_id = lambda file_name: int(match.group(0)) if (match := re.search(r'(\\d+)', file_name)) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_signature_images(\n",
    "    dataset_path: Path, image_format: List[str] = [\".png\", \".jpg\", \".jpeg\", \".bmp\"]\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "\n",
    "    Retrieves and groups the signature images with their respective signer id.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "\n",
    "    dataset_path: Path\n",
    "        Path to the signature images datasets\n",
    "    image_format: List[str]\n",
    "        The format in which the signature images are saved\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    List[Tuple[str, str]]\n",
    "        The list of signature images with their signer ids\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    images: List[Tuple[str, str]] = []\n",
    "    if not dataset_path.is_dir():\n",
    "        print(f\"Warning: Directory not found! {dataset_path}\")\n",
    "        return []\n",
    "\n",
    "    for image_path in dataset_path.iterdir():\n",
    "        if image_path.is_file() and image_path.suffix.lower() in image_format:\n",
    "            signer_id = extract_signer_id(str(image_path))\n",
    "            if signer_id != \"UKNOWN_SIGNER\":\n",
    "                images.append((signer_id, str(image_path)))\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Could not extract signer ID from file: {image_path.name}\"\n",
    "                )\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9390a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_signature_map(\n",
    "    original_signatures: List[Tuple[str, str]], forged_signatures: List[Tuple[str, str]]\n",
    ") -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "\n",
    "    Group originals and forgeries with their respective signers\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    original_signatures: List[Tuple[str, str]]\n",
    "        List of original signature images\n",
    "    forged_signatures: List[Tuple[str, str]]\n",
    "        List of forged signature images\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Dict[str, List[str]]]\n",
    "        Dictionary that groups originals and forgeries\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    signature_dictionary: defaultdict[str, Dict[str, List[Any]]] = defaultdict(\n",
    "        lambda: {\"original\": [], \"forged\": []}\n",
    "    )\n",
    "    for signer_id, image_path in original_signatures:\n",
    "        signature_dictionary[signer_id][\"original\"].append(image_path)\n",
    "\n",
    "    for signer_id, image_path in forged_signatures:\n",
    "        signature_dictionary[signer_id][\"forged\"].append(image_path)\n",
    "\n",
    "    return {k: dict(v) for k, v in signature_dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98295b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_testing_split(\n",
    "    signature_dictionary: Dict[str, Dict[str, List[str]]],\n",
    "    test_ratio: float = 0.2,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[Dict[str, Dict[str, List[str]]], Dict[str, Dict[str, List[str]]]]:\n",
    "    \"\"\"\n",
    "\n",
    "    Split the signature map into training and testing datasets.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    signature_dictionary: Dict[str, Dict[str, List[str]]]\n",
    "        Signature images mapping\n",
    "    test_ratio: float\n",
    "        Indicates the size of the test dataset\n",
    "    random_state: int\n",
    "        For reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[str, Dict[str, List[str]]], Dict[str, Dict[str, List[str]]]]\n",
    "        Contains both the training and testing mapping\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    singer_id: List[str] = sorted(list(signature_dictionary.keys()), key=int)\n",
    "\n",
    "    train_validate_signature_id: List[str]\n",
    "    test_signature_id: List[str]\n",
    "\n",
    "    train_validate_signature_id, test_signature_id = train_test_split(  # type: ignore\n",
    "        singer_id, test_size=test_ratio, random_state=random_state\n",
    "    )\n",
    "\n",
    "    def create_subset_map(signer_id_list: List[str]) -> Dict[str, Dict[str, List[str]]]:\n",
    "        subset_map: DefaultDict[str, Dict[str, List[str]]] = defaultdict(\n",
    "            lambda: {\"original\": [], \"forged\": []}\n",
    "        )\n",
    "        for signer_id in signer_id_list:\n",
    "            if signer_id in signature_dictionary:\n",
    "                subset_map[signer_id][\"original\"].extend(\n",
    "                    signature_dictionary[signer_id].get(\"original\", [])\n",
    "                )\n",
    "                subset_map[signer_id][\"forged\"].extend(\n",
    "                    signature_dictionary[signer_id].get(\"forged\", [])\n",
    "                )\n",
    "        return {k: dict(v) for k, v in subset_map.items()}\n",
    "\n",
    "    train_val_map = create_subset_map(train_validate_signature_id)  # type: ignore\n",
    "    test_map = create_subset_map(test_signature_id)  # type: ignore\n",
    "\n",
    "    return train_val_map, test_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c597b",
   "metadata": {},
   "source": [
    "### Evaluation, Plots, and Graphs\n",
    "\n",
    "Ensure that the model in `load_model_for_inference` has the same architecture as the model used for training or the model that is to be loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extraction_model():\n",
    "    \"\"\"\n",
    "\n",
    "    Model's definition\n",
    "\n",
    "    \"\"\"\n",
    "    return FeatureExtraction(\n",
    "        embedding_dim=256,\n",
    "        weights=\"IMAGENET1K_V1\",\n",
    "        use_extra_layers=True,\n",
    "        backbone_type=\"resnet18\",\n",
    "        extra_channels=[512, 256],\n",
    "        dropout_rate=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch_triplet_loss():\n",
    "    \"\"\"\n",
    "\n",
    "    Triplet loss function definition\n",
    "\n",
    "    \"\"\"\n",
    "    return BatchTripletLoss(\n",
    "        margin=0.5,\n",
    "        mining_strategy=\"batch_hard\",\n",
    "        distance_metric=\"euclidean\",\n",
    "        normalise_embeddings=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f81223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_inference(\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "    model_builder: Callable[[], nn.Module]\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "\n",
    "    Loads model for evaluation\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "\n",
    "    checkpoint_path: str\n",
    "        The path to the model\n",
    "    device: torch.device\n",
    "        'cuda' or 'cpu'\n",
    "    model_builder: Callable[[], nn.Module]\n",
    "        A function to define the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        The loaded module, ready for inference\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if not Path(checkpoint_path).exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found at: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "    model = model_builder()\n",
    "\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading checkpoint from {checkpoint_path}: {e}\")\n",
    "\n",
    "    if \"model_state_dict\" not in checkpoint:\n",
    "        raise ValueError(\n",
    "            f\"'{checkpoint_path}' does not contain 'model_state_dict'. Please ensure you saved the model's state_dict correctly\"\n",
    "        )\n",
    "\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully from {checkpoint_path} for inference.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a338906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eer_eer_threshold(\n",
    "    thresholds: npt.NDArray[np.float64],\n",
    "    fpr: npt.NDArray[np.float64],\n",
    "    fnr: npt.NDArray[np.float64],\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculate equal error rate and equal error rate threshold according to the false positive rate and false negative rate\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    thresholds: npt.NDArray[np.float64]\n",
    "        Calculated threshold Receiver Operating Characteristic\n",
    "    fpr: npt.NDArray[np.float64]\n",
    "        False positive rate\n",
    "    fnr: npt.NDArray[np.float64]\n",
    "        False negative rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]\n",
    "        Equal error rate and equal error rate threshold\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    eer_threshold: float = 0.0\n",
    "    eer: float = 0.0\n",
    "    minimum_absolute_difference: float = float(\"inf\")\n",
    "\n",
    "    for i in range(len(thresholds)):\n",
    "        absolute_difference = abs(fpr[i] - fnr[i])\n",
    "        if absolute_difference < minimum_absolute_difference:\n",
    "            minimum_absolute_difference = absolute_difference\n",
    "            eer = (fpr[i] + fnr[i]) / 2\n",
    "            eer_threshold = -thresholds[i]\n",
    "\n",
    "    return eer, eer_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d22724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(\n",
    "    all_labels_np: npt.NDArray[np.int64], all_distances_np: npt.NDArray[np.float32]\n",
    ") -> Tuple[\n",
    "    npt.NDArray[np.float64],\n",
    "    npt.NDArray[np.float64],\n",
    "    npt.NDArray[np.float64],\n",
    "    npt.NDArray[np.float64],\n",
    "    float,\n",
    "]:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculate false positive rate, true positive rate, false negative rate, threshold and roc_auc\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    all_labels_np: npt.NDArray[np.int64]\n",
    "        NumPy True or False\n",
    "    all_distances_np: npt.NDArray[np.float64]\n",
    "        Predicted distances\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], float]\n",
    "        The false positive rate, true positive rate, false negative rate, threshold and roc_auc\n",
    "    \"\"\"\n",
    "\n",
    "    inverted_distances: npt.NDArray[np.float32] = -all_distances_np\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels_np, inverted_distances)  # type: ignore\n",
    "    roc_auc = float(auc(fpr, tpr))  # type: ignore\n",
    "\n",
    "    fnr = 1 - tpr  # type: ignore\n",
    "\n",
    "    return fpr, tpr, fnr, thresholds, roc_auc  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_triplet_network(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader[Tuple[torch.Tensor, str, str]],\n",
    "    device: torch.device,\n",
    "    margin: float = 1.0,\n",
    ") -> Tuple[\n",
    "    npt.NDArray[np.float32],\n",
    "    npt.NDArray[np.int64],\n",
    "    Dict[str, npt.NDArray[np.float32]],\n",
    "    Dict[str, float],\n",
    "    Dict[str, npt.NDArray[np.float64]],\n",
    "]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Evaluate the model against a dataset.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    model: nn.Module\n",
    "        Model to be evaluated\n",
    "    test_loader: DataLoader[Tuple[torch.Tensor, str, str]\n",
    "        The dataloader must be able to be unpacked to form anchor, positive, negative, and a label\n",
    "    device: torch.device\n",
    "        'cuda' or 'cpu'\n",
    "    margin: float\n",
    "        The minimum required separation between the distance of anchor-positive pair \n",
    "        and the distance of an anchor-negative pair. \n",
    "        \n",
    "        The value must be above 0.0 \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], float]\n",
    "        The false positive rate, true positive rate, false negative rate, threshold and roc_auc\n",
    "    \"\"\"\n",
    "    \n",
    "    if margin < 0.0:\n",
    "        raise ValueError(f\"{margin} is less than 0.0\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    all_labels: List[int] = []\n",
    "    all_distances: List[float] = []\n",
    "\n",
    "    embeddings_list: Dict[str, List[npt.NDArray[np.float32]]] = {\n",
    "        \"anchor\": [],\n",
    "        \"positive\": [],\n",
    "        \"negative\": [],\n",
    "    }\n",
    "\n",
    "    total_loss: float = 0.0\n",
    "    num_batches: int = 0\n",
    "    embedding_dim: int = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative, _ in test_loader:\n",
    "\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            anchor_embed = model(anchor)\n",
    "            positive_embed = model(positive)\n",
    "            negative_embed = model(negative)\n",
    "\n",
    "            if num_batches == 0:\n",
    "                embedding_dim = anchor_embed.shape[-1]\n",
    "\n",
    "            post_dist = F.pairwise_distance(anchor_embed, positive_embed)\n",
    "            neg_dist = F.pairwise_distance(anchor_embed, negative_embed)\n",
    "\n",
    "            loss = torch.mean(torch.relu(post_dist - neg_dist + margin))\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            embeddings_list[\"anchor\"].append(anchor_embed.cpu().numpy())\n",
    "            embeddings_list[\"positive\"].append(positive_embed.cpu().numpy())\n",
    "            embeddings_list[\"negative\"].append(negative_embed.cpu().numpy())\n",
    "\n",
    "            all_distances.extend(post_dist.cpu().tolist())  # type: ignore [attr-defined]\n",
    "            all_labels.extend([1] * len(post_dist))\n",
    "\n",
    "            all_distances.extend(neg_dist.cpu().tolist())  # type: ignore [attr-defined]\n",
    "            all_labels.extend([0] * len(neg_dist))\n",
    "\n",
    "    final_embeddings: Dict[str, npt.NDArray[np.float32]] = {}\n",
    "    for key, embedding_list_for_key in embeddings_list.items():\n",
    "        if embedding_list_for_key:\n",
    "            final_embeddings[key] = np.concatenate(embedding_list_for_key, axis=0)\n",
    "        else:\n",
    "            if embedding_dim > 0:\n",
    "                final_embeddings[key] = np.empty((0, embedding_dim), dtype=np.float32)\n",
    "            else:\n",
    "                final_embeddings[key] = np.array([], dtype=np.float32).reshape(0, -1)\n",
    "\n",
    "    avg_loss: float = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    all_distances_np: npt.NDArray[np.float32] = np.array(\n",
    "        all_distances, dtype=np.float32\n",
    "    )\n",
    "    all_labels_np: npt.NDArray[np.int64] = np.array(all_labels, dtype=np.int64)\n",
    "\n",
    "    eer_threshold: float = 0.0\n",
    "    eer: float = 0.0\n",
    "\n",
    "    fpr, tpr, fnr, thresholds, roc_auc = calculate_auc(all_labels_np, all_distances_np)\n",
    "    eer, eer_threshold = calculate_eer_eer_threshold(thresholds, fpr, fnr)\n",
    "\n",
    "    avg_pos_dist = (\n",
    "        np.mean(all_distances_np[all_labels_np == 1])\n",
    "        if np.any(all_labels_np == 1)\n",
    "        else 0.0\n",
    "    )\n",
    "    avg_neg_dist = (\n",
    "        np.mean(all_distances_np[all_labels_np == 0])\n",
    "        if np.any(all_labels_np == 0)\n",
    "        else 0.0\n",
    "    )\n",
    "    \n",
    "    predicted_labels_at_eer_threshold = (all_distances_np <= eer_threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels_np, predicted_labels_at_eer_threshold)\n",
    "    precision = precision_score(all_labels_np, predicted_labels_at_eer_threshold, pos_label=1) # type: ignore\n",
    "    recall = recall_score(all_labels_np, predicted_labels_at_eer_threshold, pos_label=1) # type: ignore\n",
    "\n",
    "    metrics: Dict[str, float | Any] = {\n",
    "        \"avg_triplet_loss\": avg_loss,\n",
    "        \"avg_pos_dist\": float(avg_pos_dist),\n",
    "        \"avg_neg_dist\": float(avg_neg_dist),\n",
    "        \"auc_roc\": roc_auc,\n",
    "        \"eer\": float(eer),\n",
    "        \"eer_threshold\": float(eer_threshold),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "    \n",
    "    auc_roc: Dict[str, npt.NDArray[np.float64]] = {\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"fnr\": fnr\n",
    "    }\n",
    "\n",
    "    return all_distances_np, all_labels_np, final_embeddings, metrics, auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8940fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    all_labels_np: npt.NDArray[np.int64],\n",
    "    all_predictions_np: npt.NDArray[np.float32],\n",
    "    best_threshold: float,\n",
    ") -> None:\n",
    "\n",
    "    predictions = [1 if d <= best_threshold else 0 for d in all_predictions_np]\n",
    "    cm = confusion_matrix(all_labels_np, predictions) # type: ignore\n",
    "\n",
    "    plt.figure(figsize=(8, 6)) # type: ignore\n",
    "    sns.heatmap(cm, annot=True, fmt=\"g\", cmap=\"Blues\") # type: ignore\n",
    "    plt.title(\"Confusion Matrix\") # type: ignore\n",
    "    plt.ylabel(\"True Label\") # type: ignore\n",
    "    plt.xlabel(\"Predicted Label\") # type: ignore\n",
    "    plt.savefig(\"confusion_matrix.png\") # type: ignore\n",
    "    plt.show() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_roc(fpr: npt.NDArray[np.float64], tpr: npt.NDArray[np.float64], auc_roc: float) -> None:\n",
    "    plt.figure(figsize=(8, 6)) # type: ignore\n",
    "    plt.plot(fpr, tpr, color=\"orange\", label=f\"AUC = {auc_roc}\") # type: ignore\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\") # type: ignore\n",
    "    plt.xlabel(\"False Positive Rate\") # type: ignore\n",
    "    plt.ylabel(\"True Positive Rate\") # type: ignore\n",
    "    plt.title(\"ROC Curve\") # type: ignore\n",
    "    plt.legend(loc=\"lower right\") # type: ignore\n",
    "    plt.grid() # type: ignore\n",
    "    plt.show() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding_distances(\n",
    "    anchors: npt.NDArray[np.float32],\n",
    "    positives: npt.NDArray[np.float32],\n",
    "    negatives: npt.NDArray[np.float32],\n",
    ") -> None:\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    all_embeddings = np.concatenate([anchors, positives, negatives], axis=0)\n",
    "    embeddings_2d = pca.fit_transform(all_embeddings)  # type: ignore\n",
    "\n",
    "    n_samples = len(anchors)\n",
    "    anchors_2d = embeddings_2d[:n_samples] # type: ignore\n",
    "    positives_2d = embeddings_2d[n_samples : 2 * n_samples] # type: ignore\n",
    "    negatives_2d = embeddings_2d[2 * n_samples :] # type: ignore\n",
    "\n",
    "    plt.figure(figsize=(10, 8)) # type: ignore\n",
    "    plt.scatter(anchors_2d[:, 0], anchors_2d[:, 1], c=\"blue\", label=\"Anchors\", alpha=0.6)  # type: ignore\n",
    "    plt.scatter(positives_2d[:, 0], positives_2d[:, 1], c=\"green\", label=\"Positives\", alpha=0.6)  # type: ignore\n",
    "    plt.scatter(negatives_2d[:, 0], negatives_2d[:, 1], c=\"red\", label=\"Negatives\", alpha=0.6)  # type: ignore\n",
    "\n",
    "    for i in range(len(anchors_2d)):  # type: ignore\n",
    "        plt.plot(  # type: ignore\n",
    "            [anchors_2d[i, 0], positives_2d[i, 0]],\n",
    "            [anchors_2d[i, 1], positives_2d[i, 1]],\n",
    "            \"g-\",\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "    for i in range(len(anchors_2d)):  # type: ignore\n",
    "        plt.plot(  # type: ignore\n",
    "            [anchors_2d[i, 0], negatives_2d[i, 0]],\n",
    "            [anchors_2d[i, 1], negatives_2d[i, 1]],\n",
    "            \"r-\",\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "    plt.title(\"2D Visualization of Embedding Distances\")  # type: ignore\n",
    "    plt.xlabel(\"First Principal Component\")  # type: ignore\n",
    "    plt.ylabel(\"Second Principal Component\")  # type: ignore\n",
    "    plt.legend()  # type: ignore\n",
    "    plt.grid(True)  # type: ignore\n",
    "    plt.savefig(\"embedding_distance.png\")  # type: ignore\n",
    "    plt.show()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_embeddings_interactive(\n",
    "    anchors: npt.NDArray[np.float32],\n",
    "    positives: npt.NDArray[np.float32],\n",
    "    negatives: npt.NDArray[np.float32],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots an interactive 3D visualization of embedding distances using PCA to reduce dimensionality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    anchors : npt.NDArray[np.float32]\n",
    "        Array of anchor embeddings.\n",
    "    positives : npt.NDArray[np.float32]\n",
    "        Array of positive embeddings.\n",
    "    negatives : npt.NDArray[np.float32]\n",
    "        Array of negative embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    all_embeddings = np.concatenate([anchors, positives, negatives])\n",
    "    embeddings_3d = pca.fit_transform(all_embeddings)  # type: ignore\n",
    "\n",
    "    n_samples = len(anchors)\n",
    "    anchors_3d = embeddings_3d[:n_samples]  # type: ignore\n",
    "    positives_3d = embeddings_3d[n_samples : 2 * n_samples]  # type: ignore\n",
    "    negatives_3d = embeddings_3d[2 * n_samples :]  # type: ignore\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(  # type: ignore\n",
    "        go.Scatter3d(\n",
    "            x=anchors_3d[:, 0],\n",
    "            y=anchors_3d[:, 1],\n",
    "            z=anchors_3d[:, 2],\n",
    "            mode=\"markers\",\n",
    "            name=\"Anchors\",\n",
    "            marker=dict(size=5, color=\"blue\", opacity=0.6),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(  # type: ignore\n",
    "        go.Scatter3d(\n",
    "            x=positives_3d[:, 0],\n",
    "            y=positives_3d[:, 1],\n",
    "            z=positives_3d[:, 2],\n",
    "            mode=\"markers\",\n",
    "            name=\"Positives\",\n",
    "            marker=dict(size=5, color=\"green\", opacity=0.6),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(  # type: ignore\n",
    "        go.Scatter3d(\n",
    "            x=negatives_3d[:, 0],\n",
    "            y=negatives_3d[:, 1],\n",
    "            z=negatives_3d[:, 2],\n",
    "            mode=\"markers\",\n",
    "            name=\"Negatives\",\n",
    "            marker=dict(size=5, color=\"red\", opacity=0.6),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(  # type: ignore\n",
    "        title=\"Interactive 3D Visualization of Embedding Distances\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"First Principal Component\",\n",
    "            yaxis_title=\"Second Principal Component\",\n",
    "            zaxis_title=\"Third Principal Component\",\n",
    "        ),\n",
    "        width=800,\n",
    "        height=800,\n",
    "        margin=dict(l=0, r=0, b=0, t=30),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(  # type: ignore\n",
    "        title=\"Interactive 3D Visualization of Embedding Distances\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"First Principal Component\",\n",
    "            yaxis_title=\"Second Principal Component\",\n",
    "            zaxis_title=\"Third Principal Component\",\n",
    "            camera=dict(eye=dict(x=1.25, y=1.25, z=1.25)),\n",
    "        ),\n",
    "        width=800,\n",
    "        height=800,\n",
    "        margin=dict(l=0, r=0, b=0, t=30),\n",
    "    )\n",
    "\n",
    "    fig.show()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b96a7",
   "metadata": {},
   "source": [
    "## Training & Evaluation Loop\n",
    "\n",
    "`TrainingClass` can be used for a single training run or for k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eda1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingClass(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Combines `FeatureExtraction` class with the `BatchTripletLoss` class. It extract features from input\n",
    "    images using `ResNet`-based architecture. This class implements Mixed Precision\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    model: nn.Module\n",
    "        An instance of `FeatureExtraction` class\n",
    "    train_dataloader: Dataloader[Tuple[torch.Tensor, str, str]]\n",
    "        Signature images for the model's fine-tuning\n",
    "    learning_config: Dict[pstr, str | int | float]\n",
    "        Fine-tuning parameters (from LEARNING_CONFIG) \n",
    "    optimiser_config: Dict[str, str | Tuple[float, float] | float]\n",
    "        Optmiser's parameters (from OPTIMISER_PARAMS)\n",
    "    scheduler_config: Dict[str, str | float | int]\n",
    "        Scheduler's parameters (from SCHEDULER_PARAMS)\n",
    "    loss_function: nn.Module\n",
    "        An instance of `BatchTripletLoss` class\n",
    "    checkpoint_path: Optional[str]\n",
    "        The path in which checkpoints of the model are saved. Default is \"checkpoint\"\n",
    "    save_checkpoints: bool\n",
    "        Chooses whether checkpoints are to be saved\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "    save_checkpoint(self, epoch: int, loss: float)\n",
    "        Saves a version of the model.\n",
    "        Parameters:\n",
    "            - epoch\n",
    "            - model_state_dict\n",
    "            - optimiser_state_dict\n",
    "            - loss\n",
    "            - best_loss\n",
    "            - patience_counter\n",
    "    load_checkpoint(self, checkpoint_path)\n",
    "        Loads a version of the model.\n",
    "    train_epoch(self, dataloader: DataLoader[Tuple[torch.Tensor, torch.Tensor]]) -> float:\n",
    "        Trains the model for one epoch and returns the average training loss\n",
    "    validate_epoch(self, dataloader: DataLoader[Tuple[torch.Tensor, torch.Tensor]]) -> float\n",
    "        Validates the model and returns the average validation loss\n",
    "    fit(train_dataloder: DataLoader[Tuple[torch.Tensor, torch.Tensor]], val_dataloader: Optional[DataLoader[Tuple[torch.Tensor, torch.Tensor]]], start_epoch) -> Dict[str, Any]\n",
    "        Trains the model for a specified number of epochs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_dataloader: DataLoader[Tuple[torch.Tensor, str, str]],\n",
    "        learning_config: Dict[str, str | int | float],\n",
    "        optimiser_config: Dict[str, str | Tuple[float, float] | float],\n",
    "        scheduler_config: Dict[str, str | float | int],\n",
    "        loss_function: nn.Module,\n",
    "        checkpoint_path: Optional[str] = \"checkpoint\",\n",
    "        save_checkpoints: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__() # type: ignore\n",
    "\n",
    "        if int(learning_config.get(\"MARGIN\", 1.0)) < 0:\n",
    "            raise ValueError(\n",
    "                f\"Margin must be non-negative, got {learning_config.get('MARGIN')}\"\n",
    "            )\n",
    "        if int(learning_config.get(\"lambda_diversity\", 0.0)) < 0:\n",
    "            raise ValueError(\n",
    "                f\"lambda_diversity must be non-negative, got {learning_config.get('lambda_diversity')}\"\n",
    "            )\n",
    "\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.learning_config = learning_config\n",
    "        self.optimiser_config = optimiser_config\n",
    "        self.scheduler_config = scheduler_config\n",
    "        self.loss_function = loss_function\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.save_checkpoints = save_checkpoints\n",
    "        \n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "        self.device = torch.device(str(self.learning_config.get(\"DEVICE\", \"cpu\")))\n",
    "        self.model.to(self.device)\n",
    "        self.loss_function.to(self.device)\n",
    "\n",
    "        print(f\"Model and Loss function moved to device: {self.device}\")\n",
    "\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.patience_counter = 0\n",
    "        self.early_stopping_patience = self.learning_config.get(\n",
    "            \"EARLY_STOPPING_PATIENCE\", 10\n",
    "        )\n",
    "\n",
    "        if self.save_checkpoints:\n",
    "            self.checkpoint_path = Path(self.checkpoint_path) # type: ignore \n",
    "            self.checkpoint_path.mkdir(exist_ok=True)\n",
    "        else:\n",
    "            self.checkpoint_path = None\n",
    "\n",
    "        optimiser_name = str(self.optimiser_config.get(\"optimiser\"))\n",
    "        if not optimiser_name:\n",
    "            raise ValueError(\"Optimiser name 'optimiser' not found\")\n",
    "\n",
    "        optimiser_class = getattr(optim, optimiser_name)\n",
    "\n",
    "        optimiser_params_for_init = {\n",
    "            k: v for k, v in self.optimiser_config.items() if k != \"optimiser\"\n",
    "        }\n",
    "\n",
    "        learning_rate = self.learning_config.get(\"LEARNING_RATE\")\n",
    "        if learning_rate is None:\n",
    "            raise ValueError(\"LEARNING_RATE key missing in learning_config.\")\n",
    "\n",
    "        optimiser_params_for_init[\"lr\"] = learning_rate\n",
    "\n",
    "        self.optimiser = optimiser_class(\n",
    "            self.model.parameters(), **optimiser_params_for_init\n",
    "        )\n",
    "\n",
    "        scheduler_name = str(self.scheduler_config.get(\"scheduler\"))\n",
    "        if scheduler_name:\n",
    "            scheduler_class = getattr(lr_scheduler, scheduler_name)\n",
    "            scheduler_params_for_init = {\n",
    "                k: v for k, v in self.scheduler_config.items() if k != \"scheduler\"\n",
    "            }\n",
    "\n",
    "            if scheduler_name == \"OneCycleLR\":\n",
    "                num_epochs = self.learning_config.get(\"EPOCH\")\n",
    "                if num_epochs is None:\n",
    "                    raise ValueError(\n",
    "                        \"EPOCH key missing in LEARNING_CONFIG for OneCycleLR scheduler setup.\"\n",
    "                    )\n",
    "                if not isinstance(num_epochs, int):\n",
    "                    raise TypeError(\n",
    "                        \"EPOCH in LEARNING_CONFIG must be an integer for scheduler setup.\"\n",
    "                    )\n",
    "\n",
    "                total_steps = len(self.train_dataloader) * num_epochs\n",
    "                scheduler_params_for_init[\"total_steps\"] = total_steps\n",
    "\n",
    "            self.scheduler = scheduler_class(\n",
    "                self.optimiser, **scheduler_params_for_init\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "    def save_checkpoint(self, epoch: int, loss: float) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Saves a version of the model.\n",
    "        Parameters:\n",
    "            - epoch\n",
    "            - model_state_dict\n",
    "            - optimiser_state_dict\n",
    "            - loss\n",
    "            - best_loss\n",
    "            - patience_counter\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        epoch: int\n",
    "            The current epoch number\n",
    "        loss: float\n",
    "            The current loss value.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.save_checkpoints:\n",
    "            return\n",
    "\n",
    "        checkpoint: Dict[str, int | float | Dict[str, Any] | None] = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimiser_state_dict\": self.optimiser.state_dict(),\n",
    "            \"loss\": loss,\n",
    "            \"best_loss\": self.best_loss,\n",
    "            \"patience_counter\": self.patience_counter,\n",
    "        }\n",
    "\n",
    "        if self.scheduler:\n",
    "            checkpoint[\"scheduler_state_dict\"] = self.scheduler.state_dict()\n",
    "\n",
    "        torch.save(\n",
    "            checkpoint, self.checkpoint_path / f\"model_{epoch}_loss_{loss:.4f}.pt\" # type: ignore\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str) -> int:\n",
    "        \"\"\"\n",
    "\n",
    "        Loads a version of the model.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        checkpoint_path: str\n",
    "            Path to the checkpoint model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The training epoch number to start from.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not Path(checkpoint_path).exists():\n",
    "            raise FileNotFoundError(f\"Checkpoint file not found at: {checkpoint_path}\")\n",
    "\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading checkpoint from {checkpoint_path}: {e}\")\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        self.optimiser.load_state_dict(checkpoint[\"optimiser_state_dict\"])\n",
    "\n",
    "        if self.scheduler:\n",
    "            if (\n",
    "                \"scheduler_state_dict\" in checkpoint\n",
    "                and checkpoint[\"scheduler_state_dict\"] is not None\n",
    "            ):\n",
    "                try:\n",
    "                    self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not load scheduler state_dict. It might be incompatible. Error: {e}\"\n",
    "                    )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Warning: Scheduler initialized but no scheduler_state_dict found in checkpoint.\"\n",
    "                )\n",
    "        elif (\n",
    "            \"scheduler_state_dict\" in checkpoint\n",
    "            and checkpoint[\"scheduler_state_dict\"] is not None\n",
    "        ):\n",
    "            print(\n",
    "                \"Warning: Checkpoint contains scheduler state, but no scheduler is initialized in this \"\n",
    "                \" instance. State ignored.\"\n",
    "            )\n",
    "\n",
    "        loaded_epoch = checkpoint[\"epoch\"]\n",
    "        self.best_loss = checkpoint.get(\"best_loss\", float(\"inf\"))\n",
    "        self.patience_counter = checkpoint.get(\"patience_counter\", 0)\n",
    "\n",
    "        loaded_loss = checkpoint[\"loss\"]\n",
    "\n",
    "        print(\n",
    "            f\"Checkpoint loaded successfully: Resuming from epoch {loaded_epoch + 1}.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Loaded metrics: Last Epoch Loss = {loaded_loss:.4f}, Best Validation Loss = {self.best_loss:.4f}, Patience Counter = {self.patience_counter}\"\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        return loaded_epoch + 1\n",
    "\n",
    "    def train_epoch(\n",
    "        self, dataloader: DataLoader[Tuple[torch.Tensor, str, str]]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "\n",
    "        Trains the model for one epoch and returns the average training loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataloader: DataLoader[Tuple[torch.Tensor, torch.Tensor]]\n",
    "            The training signature images dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Average training loss calculated in the epoch.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches: int = len(dataloader)\n",
    "\n",
    "        print(\"Entering train_epoch.\")\n",
    "        for batch_index, (embedding_batch, labels_batch) in enumerate(dataloader):\n",
    "            embedding_batch = embedding_batch.to(self.device)\n",
    "            labels_batch = labels_batch.to(self.device)\n",
    "            \n",
    "            self.optimiser.zero_grad()\n",
    "            \n",
    "            with autocast('cuda'):\n",
    "                extracted_embedding = self.model(embedding_batch)\n",
    "                loss = self.loss_function(extracted_embedding, labels_batch)\n",
    "                \n",
    "            self.scaler.scale(loss).backward() #type: ignore\n",
    "            \n",
    "            grad_clip_val = self.learning_config.get(\"GRAD_CLIP\")\n",
    "            if grad_clip_val is not None:\n",
    "                self.scaler.unscale_(self.optimiser)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip_val) # type: ignore\n",
    "                \n",
    "            self.scaler.step(self.optimiser)\n",
    "            # self.optimiser.step()\n",
    "            self.scaler.update()\n",
    "            \n",
    "            if (\n",
    "                self.scheduler\n",
    "                and self.scheduler_config.get(\"scheduler\") != \"OneCycleLR\"\n",
    "            ):\n",
    "                self.scheduler.step()\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            print(\n",
    "                f\"Training batch {batch_index}/{ num_batches }, \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def validate_epoch(\n",
    "        self, dataloader: DataLoader[Tuple[torch.Tensor, str, str]]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "\n",
    "        Validates the model and returns the average validation loss\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataloader: DataLoader[Tuple[torch.Tensor, str, str]]\n",
    "            The validation signature images dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Average training loss for the epoch.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            with autocast('cuda'):\n",
    "                for _, (embeddings_batch, labels_batch) in enumerate(dataloader):\n",
    "                    embeddings_batch = embeddings_batch.to(self.device)\n",
    "                    labels_batch = labels_batch.to(self.device)\n",
    "\n",
    "                    embeddings_batch = self.model(embeddings_batch)\n",
    "\n",
    "                    loss = self.loss_function(embeddings_batch, labels_batch)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_dataloader: DataLoader[Tuple[torch.Tensor, str, str]],\n",
    "        val_dataloader: Optional[DataLoader[Tuple[torch.Tensor, str, str]]] = None,\n",
    "        start_epoch: int = 0,\n",
    "    ) -> Dict[str, List[float] | float | None]:\n",
    "        \"\"\"\n",
    "\n",
    "        Trains the model for a specified number of epochs\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        train_dataloader: DataLoader[Tuple[torch.Tensor, torch.Tensor]]\n",
    "            The training signature images dataset\n",
    "        val_dataloader: Optional[DataLoader[Tuple[torch.Tensor, torch.Tensor]]]\n",
    "            The validation signature images dataset\n",
    "        start_epoch\n",
    "            The starting epoch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            A dictionary containing:\n",
    "                - Training loss\n",
    "                - Validation loss\n",
    "                - Best validation loss\n",
    "                - Final training loss\n",
    "                - Final validation loss\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        epochs = self.learning_config.get(\"EPOCH\", 100)\n",
    "\n",
    "        scheduler_name = self.scheduler_config.get(\"scheduler\")\n",
    "\n",
    "        if scheduler_name == \"OneCycleLR\" and not self.scheduler:\n",
    "            scheduler_class = getattr(lr_scheduler, scheduler_name) # type: ignore\n",
    "            scheduler_params_for_init = {\n",
    "                k: v for k, v in self.scheduler_config.items() if k != \"scheduler\"\n",
    "            }\n",
    "            total_steps = len(train_dataloader) * int(epochs)\n",
    "            scheduler_params_for_init[\"total_steps\"] = total_steps\n",
    "            self.scheduler = scheduler_class(\n",
    "                self.optimiser, **scheduler_params_for_init\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"Starting training for {epochs} epochs from epoch {start_epoch} on device {self.device}\"\n",
    "        )\n",
    "\n",
    "        history: Dict[str, List[float] | float | None] = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"best_val_loss\": float(\"inf\"),\n",
    "            \"final_train_loss\": None,\n",
    "            \"final_val_loss\": None,\n",
    "        }\n",
    "\n",
    "        for epoch in range(start_epoch, int(epochs)):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Starting Epoch {epoch+1} training phase.\")\n",
    "\n",
    "            train_loss = self.train_epoch(train_dataloader)\n",
    "            history[\"train_loss\"].append(train_loss) # type: ignore\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "            val_loss = None\n",
    "            if val_dataloader:\n",
    "                print(f\"Starting Epoch {epoch+1} validation phase.\")\n",
    "                val_loss = self.validate_epoch(val_dataloader)\n",
    "                history[\"val_loss\"].append(val_loss) # type: ignore\n",
    "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            if self.scheduler and scheduler_name != \"OneCycleLR\":\n",
    "                self.scheduler.step(val_loss if val_loss is not None else train_loss)\n",
    "\n",
    "            # Use train_loss if no val_dataloader\n",
    "            current_val_loss = (\n",
    "                val_loss if val_dataloader else train_loss\n",
    "            )  \n",
    "\n",
    "            if current_val_loss < self.best_loss: # type: ignore\n",
    "                self.best_loss = current_val_loss\n",
    "                self.patience_counter = 0\n",
    "                history[\"best_val_loss\"] = self.best_loss\n",
    "                self.save_checkpoint(epoch, self.best_loss) # type: ignore\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                print(\n",
    "                    f\"  Patience: {self.patience_counter}/{self.early_stopping_patience}\"\n",
    "                )\n",
    "                if self.patience_counter >= int(self.early_stopping_patience):\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    break\n",
    "\n",
    "        # Store final losses\n",
    "        if history[\"train_loss\"]:\n",
    "            history[\"final_train_loss\"] = history[\"train_loss\"][-1]  # type:ignore\n",
    "        if history[\"val_loss\"]:\n",
    "            history[\"final_val_loss\"] = history[\"val_loss\"][-1]  # type:ignore\n",
    "\n",
    "        print(\"Training completed.\")\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82965c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldCrossValidator:\n",
    "    \"\"\"\n",
    "\n",
    "    This class is largely similar to `TrainingClass` in terms of attributes. The class uses `TrainingClass` to perform k-fold cross validation. \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    dataset: Dataset[Tuple[torch.Tensor, str, str]],\n",
    "        An instance of `TrainingSignatureDataset` \n",
    "    model_builder: Callable[[], nn.Module]\n",
    "        A function that creates an instance of `FeatureExtraction`\n",
    "    loss_function_builder: Callable[[], nn.Module],\n",
    "        A function that creates an instance of `BatchTripletLoss`\n",
    "    learning_config: Dict[pstr, str | int | float]\n",
    "        Fine-tuning parameters (from LEARNING_CONFIG) \n",
    "    optimiser_config: Dict[str, str | Tuple[float, float] | float]\n",
    "        Optmiser's parameters (from OPTIMISER_PARAMS)\n",
    "    scheduler_config: Dict[str, str | float | int]\n",
    "        Scheduler's parameters (from SCHEDULER_PARAMS)\n",
    "    loss_function: nn.Module\n",
    "        An instance of `BatchTripletLoss` class\n",
    "    checkpoint_path: Optional[str]\n",
    "        The path in which checkpoints of the model are saved. Default is \"kfold_checkpoints\"\n",
    "    save_checkpoints: bool\n",
    "        Chooses whether checkpoints are to be saved\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "    _aggregate_results(self) -> Dict[str, Any]:\n",
    "        Calculates the average loss value calculated on both training datasets and validation datasets\n",
    "    run_cross_validation(self) -> Dict[str, Any]:\n",
    "        Creates new dataloaders, model, and loss function in each fold.  \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset[Tuple[torch.Tensor, str, str]],\n",
    "        model_builder: Callable[[], nn.Module],\n",
    "        loss_function_builder: Callable[[], nn.Module],\n",
    "        learning_config: Dict[str, str | int | float],\n",
    "        optimiser_config: Dict[str, str | Tuple[float, float] | float],\n",
    "        scheduler_config: Dict[str, str | float | int],\n",
    "        checkpoint_directory: str = \"kfold_checkpoints\",\n",
    "        save_checkpoints: bool = False\n",
    "    ) -> None:\n",
    "        \n",
    "        self.model_builder = model_builder\n",
    "        self.dataset = dataset\n",
    "        self.loss_function_builder = loss_function_builder\n",
    "        self.learning_config = learning_config\n",
    "        self.optimiser_config = optimiser_config\n",
    "        self.scheduler_config = scheduler_config\n",
    "\n",
    "        if not callable(self.model_builder):\n",
    "            raise ValueError(\n",
    "                \"Model builder must be a callable function returning an nn.Module\"\n",
    "            )\n",
    "        if (\n",
    "            \"LEARNING_RATE\" in self.learning_config\n",
    "            and float(self.learning_config[\"LEARNING_RATE\"]) <= 0\n",
    "        ):\n",
    "            raise ValueError(\"Learning rate must be positive\")\n",
    "\n",
    "        self.save_checkpoints = save_checkpoints\n",
    "        \n",
    "        if self.save_checkpoints:\n",
    "            self.checkpoint_directory = Path(checkpoint_directory)\n",
    "            self.checkpoint_directory.mkdir(exist_ok=True)\n",
    "\n",
    "        self.k_folds_number = int(self.learning_config.get(\"K_FOLDS\", 5))\n",
    "        self.batch_size = int(self.learning_config.get(\"BATCH_SIZE\", 32))\n",
    "\n",
    "        self.k_fold = KFold(n_splits=self.k_folds_number, shuffle=True)\n",
    "\n",
    "        self.fold_results: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _aggregate_results(self) -> Dict[str, Any]:\n",
    "        average_train_losses = [\n",
    "            result[\"final_train_loss\"] \n",
    "            for result in self.fold_results \n",
    "            if \"final_train_loss\" in result and result[\"final_train_loss\"] is not None\n",
    "        ]  \n",
    "        average_validation_losses = [\n",
    "            result[\"final_val_loss\"]\n",
    "            for result in self.fold_results\n",
    "            if \"final_val_loss\" in result and result[\"final_val_loss\"] is not None\n",
    "        ]  # type:ignore\n",
    "\n",
    "        return {\n",
    "            \"mean_final_train_loss\": sum(average_train_losses)\n",
    "            / len(average_train_losses),\n",
    "            \"mean_final_validation_loss\": sum(average_validation_losses)\n",
    "            / len(average_validation_losses),\n",
    "        }\n",
    "\n",
    "    def run_cross_validation(self) -> Dict[str, Any]:\n",
    "        print(f\"--- Running {self.k_folds_number}-Fold Cross-Validation\")\n",
    "\n",
    "        for fold, (train_indices, val_indices) in enumerate(self.k_fold.split(self.dataset)): # type: ignore\n",
    "            print(f\"\\n-- Fold {fold+1//self.k_folds_number}\")\n",
    "\n",
    "            train_subset = Subset(self.dataset, train_indices) # type: ignore\n",
    "            validation_subset = Subset(self.dataset, val_indices) # type: ignore\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                train_subset, batch_size=self.batch_size, shuffle=True, num_workers=0\n",
    "            )\n",
    "            validation_loader = DataLoader(\n",
    "                validation_subset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "            )\n",
    "\n",
    "            model = self.model_builder()\n",
    "            loss_function = self.loss_function_builder()\n",
    "            print(f\"Model and Loss Function built for fold {fold+1}\")\n",
    "\n",
    "            fold_trainer = TrainingClass(\n",
    "                model=model,\n",
    "                train_dataloader=train_loader,\n",
    "                loss_function=loss_function,\n",
    "                learning_config=self.learning_config,\n",
    "                optimiser_config=self.optimiser_config,\n",
    "                scheduler_config=self.scheduler_config,\n",
    "                save_checkpoints=self.save_checkpoints,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                fold_history = fold_trainer.fit(\n",
    "                    train_dataloader=train_loader, val_dataloader=validation_loader, start_epoch=0\n",
    "                )\n",
    "                self.fold_results.append(fold_history)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during fold {fold + 1} training: {e}\")\n",
    "                self.fold_results.append({\"error\": str(e)})\n",
    "\n",
    "        aggregated_metrics = self._aggregate_results()\n",
    "        print(\"\\n--- K-Fold Cross-Validation Complete ---\")\n",
    "        return aggregated_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d56c2e",
   "metadata": {},
   "source": [
    "# Data Loading & Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7b394",
   "metadata": {},
   "source": [
    "## Load Raw Data Map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path: Path = Path(DATASET_CONFIG[\"DATASET_PATH\"])\n",
    "original_signatures = retrieve_signature_images(dataset_path / \"original\")\n",
    "forged_signatures = retrieve_signature_images(dataset_path / \"forged\")\n",
    "signature_map = prepare_signature_map(original_signatures, forged_signatures)\n",
    "train_val_signatures_map, test_signatures_map = training_and_testing_split(\n",
    "    signature_map, test_ratio=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7befef32",
   "metadata": {},
   "source": [
    "## Define Transforms\n",
    "\n",
    "For more variety, consider addding `RandomRotation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=(-5, 5)\n",
    "        ),\n",
    "\n",
    "        transforms.RandomResizedCrop(\n",
    "            (224, 224), scale=(0.9, 1.05), ratio=(0.95, 1.05), antialias=True\n",
    "        ),\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab44d0e",
   "metadata": {},
   "source": [
    "## Instantiate Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainingSignatureDataset(\n",
    "    data_map=train_val_signatures_map, transform=train_transform\n",
    ")\n",
    "\n",
    "test_dataset = TestingSignatureDataset(\n",
    "    data_map=test_signatures_map, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1eb7a",
   "metadata": {},
   "source": [
    "## Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a45cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, LEARNING_CONFIG[\"BATCH_SIZE\"], shuffle=False) # type: ignore\n",
    "train_dataloader = DataLoader(train_dataset, LEARNING_CONFIG[\"BATCH_SIZE\"], shuffle=True) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f77130",
   "metadata": {},
   "source": [
    "# Model Initialisation & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = build_feature_extraction_model\n",
    "loss_function_builder = build_batch_triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_validator = KFoldCrossValidator(\n",
    "    dataset=train_dataset,\n",
    "    model_builder=model_builder,\n",
    "    loss_function_builder=loss_function_builder,\n",
    "    learning_config=LEARNING_CONFIG,\n",
    "    optimiser_config=OPTIMISER_PARAMS,\n",
    "    scheduler_config=SCHEDULER_PARAMS,\n",
    "    checkpoint_directory=\"kfold_checkpoints\",\n",
    "    save_checkpoints=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_trainer = TrainingClass(\n",
    "    model=model_builder(),\n",
    "    train_dataloader=train_dataloader,\n",
    "    loss_function=loss_function_builder(),\n",
    "    learning_config=LEARNING_CONFIG,\n",
    "    optimiser_config=OPTIMISER_PARAMS,\n",
    "    scheduler_config=SCHEDULER_PARAMS,\n",
    "    save_checkpoints=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ede324",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9779c",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e066a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_result = k_fold_validator.run_cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2fb7d",
   "metadata": {},
   "source": [
    "## Training Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df706b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = final_model_trainer.fit(train_dataloader=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a1fc2",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Load the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241baeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_evaluation = load_model_for_inference(\"model/model_29_loss_0.0510.pt\", LEARNING_CONFIG[\"DEVICE\"], model_builder) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3ca81",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_for_evaluation = TestingSignatureDataset(\n",
    "    data_map=train_val_signatures_map, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfcf09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_for_evaluation = DataLoader(\n",
    "    train_dataset_for_evaluation, batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_distances, train_all_labels, train_embeddings, train_metrics, auc_roc_metrics = (\n",
    "    evaluate_triplet_network(\n",
    "        model_for_evaluation,\n",
    "        train_dataloader_for_evaluation,\n",
    "        LEARNING_CONFIG[\"DEVICE\"], # type: ignore\n",
    "        0.5,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951f984",
   "metadata": {},
   "source": [
    "## Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_distances, test_all_labels, test_embeddings, test_metrics, auc_roc_metrics = (\n",
    "    evaluate_triplet_network(\n",
    "        model_for_evaluation, test_dataloader, LEARNING_CONFIG[\"DEVICE\"], 0.5 # type: ignore\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78185f2f",
   "metadata": {},
   "source": [
    "# Plots, and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8ebf7",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_roc(auc_roc_metrics[\"fpr\"], auc_roc_metrics[\"tpr\"], train_metrics[\"auc_roc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(train_all_labels, train_all_distances, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_distances(\n",
    "    train_embeddings[\"anchor\"],\n",
    "    train_embeddings[\"positive\"],\n",
    "    train_embeddings[\"negative\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bfb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_embeddings_interactive(\n",
    "    train_embeddings[\"anchor\"],\n",
    "    train_embeddings[\"positive\"],\n",
    "    train_embeddings[\"negative\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a80a68",
   "metadata": {},
   "source": [
    "## Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8637400",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_roc(auc_roc_metrics[\"fpr\"], auc_roc_metrics[\"tpr\"], test_metrics[\"auc_roc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_all_labels, test_all_distances, 1.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_distances(test_embeddings['anchor'], test_embeddings['positive'], test_embeddings['negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_embeddings_interactive(test_embeddings['anchor'], test_embeddings['positive'], test_embeddings['negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0141334",
   "metadata": {},
   "source": [
    "# Utilising The Model\n",
    "\n",
    "For the model to work, the signature images should be preprocessed the same way it was used during fine-tuning. \n",
    "\n",
    "The following demonstration uses the testing dataset:\n",
    "```bash\n",
    "Signer ids: ['6', '13', '14', '20', '27', '32', '33', '42', '44', '50', '53']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0777206",
   "metadata": {},
   "source": [
    "## Preparing signature images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_signature_image_path: Path = Path(\n",
    "    \"processed_signature_images/original/original_53_9.png\"\n",
    ")\n",
    "original_input_signature_path: Path = Path(\n",
    "    \"processed_signature_images/original/original_53_22.png\"\n",
    ")\n",
    "original_input_different_signer: Path = Path(\n",
    "    \"processed_signature_images/original/original_27_12.png\"\n",
    ")\n",
    "forged_input_same_signer: Path = Path(\n",
    "    \"processed_signature_images/forged/forgeries_53_13.png\"\n",
    ")\n",
    "forged_input_different_signer: Path = Path(\n",
    "    \"processed_signature_images/forged/forgeries_50_5.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83356c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_image = Image.open(str(reference_signature_image_path)).convert(\"L\")\n",
    "input_image = Image.open(str(original_input_signature_path)).convert(\"L\")\n",
    "forged_same_signer = Image.open(str(forged_input_same_signer)).convert(\"L\")\n",
    "forged_different_signer_image = Image.open(str(forged_input_different_signer)).convert(\n",
    "    \"L\"\n",
    ")\n",
    "original_different_signer_image = Image.open(\n",
    "    str(original_input_different_signer)\n",
    ").convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tensor = test_transform(reference_image)  # type: ignore\n",
    "input_tensor = test_transform(input_image)  # type: ignore\n",
    "forged_same_signer_tensor = test_transform(forged_same_signer)  # type: ignore\n",
    "forged_different_signer_tensor = test_transform(forged_different_signer_image)  # type: ignore\n",
    "original_different_signer_tensor = test_transform(original_different_signer_image)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49323dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tensor = reference_tensor.unsqueeze(0)  # type: ignore\n",
    "input_tensor = input_tensor.unsqueeze(0)  # type: ignore\n",
    "forged_same_signer_tensor = forged_same_signer_tensor.unsqueeze(0)  # type: ignore\n",
    "forged_different_signer_tensor = forged_different_signer_tensor.unsqueeze(0)  # type: ignore\n",
    "original_different_signer_tensor = original_different_signer_tensor.unsqueeze(0)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tensor = reference_tensor.to(\"cuda\")  # type: ignore\n",
    "input_tensor = input_tensor.to(\"cuda\")  # type: ignore\n",
    "forged_same_signer_tensor = forged_same_signer_tensor.to(\"cuda\")  # type: ignore\n",
    "forged_different_signer_tensor = forged_different_signer_tensor.to(\"cuda\")  # type: ignore\n",
    "original_different_signer_tensor = original_different_signer_tensor.to(\"cuda\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b25046",
   "metadata": {},
   "source": [
    "## Applying The Model\n",
    "\n",
    "Relying purely on the model itself is prone to misclassifications in edge cases, so I implemented cosine similarity to complement the model. \n",
    "\n",
    "Cosine similariy will measure how similar two vectors are, based on the angle between them rather than their magnitude. \n",
    "1. If the two vectors point in the same direction, similarity score is close to 1.0,\n",
    "2. If the two vectors are orthogonal, simmilarity score is 0.0, indicating an absence of similarity\n",
    "3. Opposite vectors have a similarity score near -1.0, signifying total dissimilarity.\n",
    "\n",
    "As for the distance, I implemented a simple Euclidean distance calculation. \n",
    "\n",
    "Additionally, a confidence score derived from the similarity score and the distance is also calculated. \n",
    "\n",
    "*The more time you spend thinking about this, the more you realise the amount of additional work needed to make it a complete system. So, I will only keep this example simple.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(\n",
    "    reference_tensor: torch.Tensor,\n",
    "    input_tensor: torch.Tensor,\n",
    "    forged_same_signer_tensor: torch.Tensor,\n",
    "    forged_different_signer_tensor: torch.Tensor,\n",
    "    original_different_signer_tensor: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    reference_feature_embeddings = torch.Tensor(model_for_evaluation(reference_tensor))\n",
    "    input_feature_embeddings = torch.Tensor(model_for_evaluation(input_tensor))\n",
    "    forged_same_signer_feature_embeddings = torch.Tensor(model_for_evaluation(\n",
    "        forged_same_signer_tensor\n",
    "    ))\n",
    "    forged_different_signer_feature_embeddings = torch.Tensor(model_for_evaluation(\n",
    "        forged_different_signer_tensor\n",
    "    ))\n",
    "    original_different_signer_feature_embeddings = torch.Tensor(model_for_evaluation(\n",
    "        original_different_signer_tensor\n",
    "    ))\n",
    "    return (\n",
    "        reference_feature_embeddings,\n",
    "        input_feature_embeddings,\n",
    "        forged_same_signer_feature_embeddings,\n",
    "        forged_different_signer_feature_embeddings,\n",
    "        original_different_signer_feature_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ba196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    reference_feature_embeddings: torch.Tensor, input_feature_embeddings: torch.Tensor\n",
    ") -> Tuple[float, float, float, float, float]:\n",
    "    similarity = F.cosine_similarity(\n",
    "        reference_feature_embeddings, input_feature_embeddings\n",
    "    ).item()\n",
    "\n",
    "    distance = F.pairwise_distance(\n",
    "        reference_feature_embeddings, input_feature_embeddings\n",
    "    ).item()\n",
    "\n",
    "    normalised_distance = max(0, min(1, distance / 1.3))\n",
    "    distance_score = 1 - normalised_distance\n",
    "\n",
    "    # Greater emphasis is placed on the distance calculated.\n",
    "    confidence_score = 0.8 * similarity + 0.2 * distance_score\n",
    "\n",
    "    return similarity, confidence_score, normalised_distance, distance_score, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(\n",
    "    similarity: float, \n",
    "    distance: float,\n",
    "    confidence_score: float,\n",
    "    result: Dict[str, bool | float | str]\n",
    ") -> None:\n",
    "    pass_thresholds = (\n",
    "        similarity >= 0.5 and \n",
    "        distance <= 1.07\n",
    "    )\n",
    "    \n",
    "    if pass_thresholds:\n",
    "        if confidence_score >= 0.9:\n",
    "            result['prediction_level'] = 'Very High Confidence'\n",
    "            result['is_genuine'] = True\n",
    "        elif confidence_score >= 0.8:\n",
    "            result['prediction_level'] = 'High Confidence'\n",
    "            result['is_genuine'] = True\n",
    "        elif confidence_score >= 0.7:\n",
    "            result['prediction_level'] = 'Medium Confidence'\n",
    "            result['is_genuine'] = True\n",
    "        elif confidence_score >= 0.6:\n",
    "            result['prediction_level'] = 'Low Confidence'\n",
    "        else:\n",
    "            result['prediction_level'] = \"Very Low Confidence\"\n",
    "    else:\n",
    "        result['prediction_level'] = 'Failed Threshold Check'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    reference_feature_embeddings,\n",
    "    input_feature_embeddings,\n",
    "    forged_same_signer_feature_embeddings,\n",
    "    forged_different_signer_feature_embeddings,\n",
    "    original_different_signer_feature_embeddings,\n",
    ") = apply_model(\n",
    "    reference_tensor,  # type: ignore\n",
    "    input_tensor,  # type: ignore\n",
    "    forged_same_signer_tensor,  # type: ignore\n",
    "    forged_different_signer_tensor,  # type: ignore\n",
    "    original_different_signer_tensor,  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae891aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image_tensor(\n",
    "    tensor: torch.Tensor, \n",
    "    mean: float, \n",
    "    std: float\n",
    "    ) -> torch.Tensor: \n",
    "    \n",
    "    mean_tensor = torch.tensor(mean).view(-1, 1, 1).to(tensor.device)\n",
    "    std_tensor = torch.tensor(std).view(-1, 1, 1).to(tensor.device)\n",
    "    denormalized_tensor = tensor * std_tensor + mean_tensor\n",
    "    \n",
    "    return torch.clamp(denormalized_tensor, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1797b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_image(reference_tensor: torch.Tensor, input_tensor: torch.Tensor):\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    \n",
    "    reference_signature = denormalize_image_tensor(reference_tensor.to('cuda'), 0.5, 0.5).squeeze(0)\n",
    "    input_signature = denormalize_image_tensor(input_tensor.to('cuda'), 0.5, 0.5).squeeze(0)\n",
    "    \n",
    "    _, axes = plt.subplots(1, 2, figsize=(15, 5)) # type: ignore\n",
    "\n",
    "    axes[0].imshow(to_pil(reference_signature), cmap='gray')\n",
    "    axes[0].set_title(f\"Reference\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(to_pil(input_signature), cmap='gray')\n",
    "    axes[1].set_title(f\"Input\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # type: ignore\n",
    "    plt.show() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded129a2",
   "metadata": {},
   "source": [
    "### Same signer (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_image(torch.Tensor(reference_tensor), torch.Tensor(input_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443385e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(similarity, confidence_score, normalised_distance, distance_score, distance) = (\n",
    "    calculate_metrics(reference_feature_embeddings, input_feature_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_result: Dict[str, bool |  float | str] = {\n",
    "    'is_genuine': False,\n",
    "    'confidence_score': confidence_score,\n",
    "    'similarity_score': similarity,\n",
    "    'euclidean_distance': normalised_distance,\n",
    "    'distance_score': distance_score,\n",
    "    'prediction_level': 'unknown',\n",
    "    'passed_thresholds': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_threshold(\n",
    "    float(first_result[\"similarity_score\"]),\n",
    "    float(first_result[\"distance_score\"]),\n",
    "    float(first_result[\"confidence_score\"]),\n",
    "    first_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d663543",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591c6f4",
   "metadata": {},
   "source": [
    "### Same Signer (Forged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_image(torch.Tensor(reference_tensor), torch.Tensor(forged_same_signer_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(similarity, confidence_score, normalised_distance, distance_score, distance) = (\n",
    "    calculate_metrics(reference_feature_embeddings, forged_same_signer_feature_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a81fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_result: Dict[str, bool |  float | str] = {\n",
    "    'is_genuine': False,\n",
    "    'confidence_score': confidence_score,\n",
    "    'similarity_score': similarity,\n",
    "    'euclidean_distance': normalised_distance,\n",
    "    'distance_score': distance_score,\n",
    "    'prediction_level': 'unknown',\n",
    "    'passed_thresholds': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584619f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_threshold(\n",
    "    float(second_result[\"similarity_score\"]),\n",
    "    float(second_result[\"distance_score\"]),\n",
    "    float(second_result[\"confidence_score\"]),\n",
    "    second_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d21e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c036db",
   "metadata": {},
   "source": [
    "### Different Signer (Forged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6275ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_image(torch.Tensor(reference_tensor), torch.Tensor(forged_different_signer_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eab22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(similarity, confidence_score, normalised_distance, distance_score, distance) = (\n",
    "    calculate_metrics(reference_feature_embeddings, forged_different_signer_feature_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c01c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_result: Dict[str, bool |  float | str] = {\n",
    "    'is_genuine': False,\n",
    "    'confidence_score': confidence_score,\n",
    "    'similarity_score': similarity,\n",
    "    'euclidean_distance': normalised_distance,\n",
    "    'distance_score': distance_score,\n",
    "    'prediction_level': 'unknown',\n",
    "    'passed_thresholds': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_threshold(\n",
    "    float(third_result[\"similarity_score\"]),\n",
    "    float(third_result[\"distance_score\"]),\n",
    "    float(third_result[\"confidence_score\"]),\n",
    "    third_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c675229",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f2e82",
   "metadata": {},
   "source": [
    "### Different Signer (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_image(torch.Tensor(reference_tensor), torch.Tensor(original_different_signer_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "(similarity, confidence_score, normalised_distance, distance_score, distance) = (\n",
    "    calculate_metrics(reference_feature_embeddings, original_different_signer_feature_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_result: Dict[str, bool |  float | str] = {\n",
    "    'is_genuine': False,\n",
    "    'confidence_score': confidence_score,\n",
    "    'similarity_score': similarity,\n",
    "    'euclidean_distance': normalised_distance,\n",
    "    'distance_score': distance_score,\n",
    "    'prediction_level': 'unknown',\n",
    "    'passed_thresholds': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f15421",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_threshold(\n",
    "    float(fourth_result[\"similarity_score\"]),\n",
    "    float(fourth_result[\"distance_score\"]),\n",
    "    float(fourth_result[\"confidence_score\"]),\n",
    "    fourth_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
